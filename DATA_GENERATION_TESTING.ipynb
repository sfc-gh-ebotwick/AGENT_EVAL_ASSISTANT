{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lastEditStatus": {
   "notebookId": "chgign3iaigs43ykwi4q",
   "authorId": "5095547476787",
   "authorName": "EBOTWICK",
   "authorEmail": "elliott.botwick@snowflake.com",
   "sessionId": "549cf842-6180-4378-91b8-56db4e0f3a5d",
   "lastEditTime": 1763014935287
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "a1ee390b-c216-4c30-9cef-15e61b5a1acd",
   "metadata": {
    "language": "python",
    "name": "python_import"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark as snowpark\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.cortex import complete\n\nfrom typing import List\nimport os\nimport sys\nimport json\nimport time\nimport requests\n\n#Set up snowflake session vars and env vars\nsession = get_active_session()\nsession",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84fc2316-f36a-4a07-b467-e34e5d2ed96f",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "SELECT RECORD_ATTRIBUTES:\"ai.observability.record_id\", * FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS\n    WHERE SCOPE:name = 'snow.cortex.agent'\n    ORDER BY TIMESTAMP DESC\n    LIMIT 250;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd3e4ef6-23db-4c33-a516-d9586fffcf71",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "WITH RESULTS AS (SELECT \n    TIMESTAMP AS TS,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.object.name\" AS AGENT_NAME,\n    RECORD_ATTRIBUTES:\"ai.observability.record_id\" AS RECORD_ID,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.thread_id\" AS THREAD_ID,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.parent_message_id\" AS PARENT_MESSAGE_ID,\n    COALESCE(\n        PARENT_MESSAGE_ID,\n        LAST_VALUE (PARENT_MESSAGE_ID) IGNORE NULLS OVER (\n          PARTITION BY AGENT_NAME,\n          THREAD_ID\n          ORDER BY\n            TIMESTAMP ROWS BETWEEN UNBOUNDED PRECEDING\n            AND CURRENT ROW\n        )\n      ) AS PARENT_MESSAGE,\n\n    CONCAT(THREAD_ID, '-', PARENT_MESSAGE) AS THREAD_MESSAGE_ID,\n\n    -- VALUE:\"snow.ai.observability.request_body\".\"messages\"[0].\"content\"[0].\"text\" AS INPUT_QUERY,\n    RECORD_ATTRIBUTES:\"ai.observability.record_root.input\" AS INPUT_QUERY,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.thinking_response\" AS AGENT_PLANNING,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_analyst.sql_query\" AS GENERATED_SQL,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.sql_execution.result\" AS SQL_RESULT,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_search.results\" AS CORTEX_SEARCH_RESULT,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_search.name\" AS CORTEX_SEARCH_SERVICE,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.custom_tool.results\" AS CUSTOM_TOOL_RESULT,\n    RECORD_ATTRIBUTES:\"ai.observability.record_root.output\" AS AGENT_RESPONSE, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.model\" AS REASONING_MODEL, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool.name\" AS AVAILABLE_TOOLS, \n    -- CASE \n    --     WHEN RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool_selection.name\" IS NOT NULL\n    --     THEN RECORD:\"name\" \n    --     ELSE NULL END\n    -- RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool_selection.name\" AS TOOL_SELECTION,\n    RECORD:\"name\" as TOOL_CALL,\n\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool_selection.type\" AS TOOL_TYPE,\n    CASE \n        WHEN RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.id\" IS NOT NULL\n        THEN OBJECT_CONSTRUCT (\n            'TOOL_NAME',\n            TOOL_CALL,\n            'GENERATED_SQL',\n            GENERATED_SQL,\n            'CORTEX_SEARCH_RESULT',\n            CORTEX_SEARCH_RESULT,\n            'CUSTOM_TOOL_RESULT',\n            CUSTOM_TOOL_RESULT\n            )\n        ELSE NULL\n        END AS TOOL_ARRAY,\n\n    CASE\n        WHEN VALUE:\"positive\"='true' THEN 1\n        WHEN VALUE:\"positive\"='false'THEN 0\n        ELSE NULL\n        END AS USER_FEEDBACK,\n    VALUE:\"feedback_message\" AS USER_FEEDBACK_MESSAGE,\n    RECORD_ATTRIBUTES\n    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS \n    WHERE SCOPE:name = 'snow.cortex.agent'\n    AND RECORD:\"name\" NOT IN ('SqlExecution', 'SqlExecution_CortexAnalyst','CortexChartToolImpl-data_to_chart')\n    AND AGENT_NAME = 'GENOMICS_AGENT'\n    -- AND OPERATION !='Agent'\n    -- AND THREAD_ID ='475131216317'\n    -- AND RECORD_ID = '8f57c963-a79e-4788-8d32-ecfdf730c840'\n    -- AND USER_FEEDBACK IS NOT NULL\n    -- AND TOOL_SELECTION IS NOT NULL\n    -- AND RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.id\" IS NOT NULL\n    -- 'toolu_bdrk_01WjbFWyvYw2jvxd9dn6irNV'\n    ORDER BY TIMESTAMP DESC, START_TIMESTAMP DESC, THREAD_ID)\n\n    SELECT * FROM RESULTS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4c74a07-b685-4121-9ffb-f75a39151977",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "WITH RESULTS AS (SELECT \n    TIMESTAMP AS TS,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.object.name\" AS AGENT_NAME,\n    RECORD_ATTRIBUTES:\"ai.observability.record_id\" AS RECORD_ID, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.thread_id\" AS THREAD_ID,\n    RECORD_ATTRIBUTES:\"ai.observability.record_root.input\" AS INPUT_QUERY,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.thinking_response\" AS AGENT_PLANNING,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_analyst.sql_query\" AS GENERATED_SQL,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.sql_execution.result\" AS SQL_RESULT,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_search.results\" AS CORTEX_SEARCH_RESULT,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.custom_tool.results\" AS CUSTOM_TOOL_RESULT,\n    RECORD_ATTRIBUTES:\"ai.observability.record_root.output\" AS AGENT_RESPONSE, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.model\" AS REASONING_MODEL, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool.name\" AS AVAILABLE_TOOLS, \n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool_selection.name\" AS TOOL_SELECTION,\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.cortex_search.name\" AS CSS_NAME,\n\n    RECORD:\"name\" as TOOL_CALL,\n\n    RECORD_ATTRIBUTES:\"snow.ai.observability.agent.planning.tool_selection.type\" AS TOOL_TYPE,\n    CASE \n        WHEN RECORD_ATTRIBUTES:\"snow.ai.observability.agent.tool.id\" IS NOT NULL     \n        AND RECORD:\"name\" NOT IN ('SqlExecution', 'SqlExecution_CortexAnalyst','CortexChartToolImpl-data_to_chart')\n \n        THEN OBJECT_CONSTRUCT (\n            'tool_name',\n            TOOL_CALL,\n            'tool_output',\n            OBJECT_CONSTRUCT(\n            'SQL',\n            GENERATED_SQL,\n            'search results',\n            CORTEX_SEARCH_RESULT,\n            'CUSTOM_TOOL_RESULT',\n            CUSTOM_TOOL_RESULT\n            ))\n        ELSE NULL\n        END AS TOOL_ARRAY,\n\n    CASE\n        WHEN VALUE:\"positive\"='true' THEN 1\n        WHEN VALUE:\"positive\"='false'THEN 0\n        ELSE NULL\n        END AS USER_FEEDBACK,\n    VALUE:\"feedback_message\" AS USER_FEEDBACK_MESSAGE,\n\n    RECORD:\"name\" as OPERATION,\n    *\n    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS \n    WHERE SCOPE:name = 'snow.cortex.agent'\n    -- AND AGENT_NAME = 'FINANCE_AGENT'\n    AND OPERATION !='Agent'\n    -- AND THREAD_ID = '475131216309'\n    ORDER BY THREAD_ID, TIMESTAMP, START_TIMESTAMP ASC)\n\n    SELECT \n        RECORD_ID,\n        MIN(TIMESTAMP) AS START_TS,\n        MAX(TIMESTAMP) AS END_TS,\n        DATEDIFF(SECOND, START_TS, END_TS)::FLOAT AS LATENCY, \n        MIN(AGENT_NAME) AS AGENT_NAME,\n        MIN(INPUT_QUERY) AS INPUT_QUERY,\n        MIN(AGENT_RESPONSE) AS AGENT_RESPONSE,\n        MIN(AGENT_PLANNING) AS AGENT_PLANNING,\n        ARRAY_AGG(TOOL_SELECTION)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS TOOL_CALLS,\n        ARRAY_AGG(CSS_NAME)   WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS CSS_CALLS,\n        ARRAY_AGG(TOOL_TYPE)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS TOOL_TYPES,\n        ARRAY_AGG(GENERATED_SQL)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS GENERATED_SQLS,\n        -- ARRAY_AGG(SQL_RESULT)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS SQL_RESULTS,\n        ARRAY_AGG(CORTEX_SEARCH_RESULT)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS CORTEX_SEARCH_RESULTS,\n        ARRAY_AGG(CUSTOM_TOOL_RESULT)  WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS CUSTOM_TOOL_RESULTS,\n        -- OBJECT_AGG('GENERATED_SQL', GENERATED_SQL,                              \n        -- 'SQL_RESULT', SQL_RESULT,                                  \n        -- 'CORTEX_SEARCH_RESULT', CORTEX_SEARCH_RESULT, \n        -- 'CUSTOM_TOOL_RESULT', CUSTOM_TOOL_RESULT)  AS TOOL_RESULTS,\n\n        -- ARRAY_AGG (\n          -- OBJECT_CONSTRUCT (\n          --   'tool_type',\n          --   TOOL_TYPE,\n          --   'tool_name',\n          --   TOOL_SELECTION,\n          --   'generated_sql',\n          --   GENERATED_SQL,\n          --   'SQL_RESULT',\n          --   SQL_RESULT,\n          --   'CORTEX_SEARCH_RESULT',\n          --   CORTEX_SEARCH_RESULT,\n          --   'CUSTOM_TOOL_RESULT',\n          --   CUSTOM_TOOL_RESULT\n          --           )\n        --         ) \n        --     WITHIN GROUP (\n        --       ORDER BY\n        --         TIMESTAMP ASC\n        --     ) \n        --     AS TOOL_RESULTS,\n\n        -- ARRAY_AGG(COALESCE(GENERATED_SQL, CORTEX_SEARCH_RESULT, CUSTOM_TOOL_RESULT))  \n        -- WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS TOOL_RESULTS,\n        ARRAY_AGG(TOOL_ARRAY) WITHIN GROUP (ORDER BY TIMESTAMP ASC) AS TOOL_ARRAY,\n        MIN(USER_FEEDBACK) AS USER_FEEDBACKS,\n        MIN(USER_FEEDBACK_MESSAGE) AS USER_FEEDBACK_MESSAGES\n    \n        FROM RESULTS\n        -- WHERE RECORD_ID ilike '%8f57c963-a79e-4788-8d32-ecfdf730c840%'\n    \n        GROUP BY RECORD_ID\n        -- HAVING USER_FEEDBACKS[0] IS NOT NULL\n        ORDER BY START_TS DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3aee8b3-18e0-419e-9651-3adc6a5d662d",
   "metadata": {
    "language": "python",
    "name": "convert_query_cell_to_pandas"
   },
   "outputs": [],
   "source": "import json\n\ndf = cell6.to_pandas()\ndf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f208e4-2aaa-4f52-8fae-bc695565b3b8",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# df.drop_duplicates(subset=['AGENT_NAME', 'INPUT_QUERIES'], inplace=True)\ndf.shape",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "946ebcf3-6ad7-4c1d-b989-fa9aed038423",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "# [print(idx, tool.update({'SEQ', idx})) for idx, tool in enumerate(b)]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "754a32a2-964f-45b3-857e-df0b981ad1aa",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "b = df.TOOL_ARRAY.apply(lambda x: ast.literal_eval(x))[33]\n\ndef add_tool_sequence(tool_list):\n\n    for idx, tool in enumerate(tool_list):\n        tool.update({'tool_sequence': idx+1})\n        \n    new_order = ['tool_sequence', 'tool_name', 'tool_output']\n\n    # final_tool_list = {k: tool_list[k] for k in new_order if k in tool_list}\n\n     # return a new list where each dict's keys appear in new_order\n    final_tool_list = [\n        {k: tool[k] for k in new_order if k in tool}\n        for tool in tool_list\n    ]\n    return final_tool_list\n        \n\nadd_tool_sequence(b)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82beb7d5-d0b5-4542-9028-ba76571205a4",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "df['TOOL_CALLING']  = df.TOOL_ARRAY.apply(lambda x: add_tool_sequence(ast.literal_eval(x)))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99eaf362-a7aa-4394-99ab-5df1f0e9ac66",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "import ast \n\ndef convert_tools_to_sequence(tool_list):\n    \"\"\"\n    Convert a list of tool names into a list of dictionaries with tool names and sequence numbers\n    \n    Args:\n        tool_list (list): List of tool names\n        \n    Returns:\n        list: List of dictionaries containing tool_name and tool_sequence\n    \"\"\"\n    return [\n        {\n            'tool_name': ast.literal_eval(tool)[0],\n            'tool_sequence': str(idx + 1)\n        }\n        for idx, tool in enumerate(tool_list)\n    ]\n\ndf['TOOL_SELECTION'] = df.TOOL_CALLS.apply(lambda x: json.dumps(convert_tools_to_sequence(ast.literal_eval(x))))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f912470e-aab0-4825-8c97-5a62e81b9c76",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SHOW TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcf7dee3-9d30-424a-9889-b138adb61910",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "final_df = df[['RECORD_ID', 'START_TS', 'AGENT_NAME',\n              'INPUT_QUERY', 'AGENT_RESPONSE', 'TOOL_CALLING', \n               'LATENCY','USER_FEEDBACKS', 'USER_FEEDBACK_MESSAGES',]]\n\nsession.write_pandas(final_df, 'PARSED_AI_OBS_LOGS', auto_create_table=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f6e6fb9-7c08-4e2a-b817-c77541116aff",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "import ast \n\ndef create_tool_execution_sequence(df):\n    \"\"\"\n    Convert a list of tool names into a list of dictionaries with tool names and sequence numbers\n    \n    Args:\n        tool_list (list): List of tool names\n        \n    Returns:\n        list: List of dictionaries containing tool_name and tool_sequence\n    \"\"\"\n    return [\n        {\n            'tool_name': ast.literal_eval(tool)[0],\n            'tool_sequence': str(idx + 1)\n        }\n        for idx, tool in enumerate(tool_list)\n    ]\n\n# df['TOOL_SELECTION'] = df.TOOL_CALLS.apply(lambda x: json.dumps(convert_tools_to_sequence(ast.literal_eval(x))))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cc7c455-78e8-45a6-ab63-b40a165a4c19",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE ai_observability_processed\nTARGET_LAG = '1 hour'\nWAREHOUSE = 'MEDIUM'\nAS\nWITH ai_flat AS (\n    SELECT \n        TIMESTAMP,\n        START_TIMESTAMP,\n        OBSERVED_TIMESTAMP,\n        TRACE,\n        RESOURCE_ATTRIBUTES,\n        SCOPE,\n        RECORD_TYPE,\n        RECORD,\n        RECORD_ATTRIBUTES,\n        VALUE,\n        RESOURCE_ATTRIBUTES:snow.database.name::STRING AS database_name,\n        RESOURCE_ATTRIBUTES:snow.schema.name::STRING AS schema_name,\n        RESOURCE_ATTRIBUTES:snow.user.name::STRING AS user_name,\n        RESOURCE_ATTRIBUTES:snow.warehouse.name::STRING AS warehouse_name,\n        TRACE:trace_id::STRING AS trace_id,\n        TRACE:span_id::STRING AS span_id,\n        RECORD:name::STRING AS event_name,\n        RECORD:severity_text::STRING AS severity_level\n    FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS\n),\nbase_data AS (\n    SELECT\n        event_name,\n        observed_timestamp,\n        start_timestamp,\n        timestamp,\n        RECORD_ATTRIBUTES['snow.ai.observability.object.name']::STRING AS agent_name,\n        RECORD_ATTRIBUTES['snow.ai.observability.user.name']::STRING AS username,\n        RECORD_ATTRIBUTES['snow.ai.observability.agent.thread_id']::STRING AS conversation_id,\n        RECORD_ATTRIBUTES['ai.observability.record_id']::STRING AS record_id,\n        VALUE['snow.ai.observability.response']::STRING AS ai_response,\n        VALUE['snow.ai.observability.response_time_ms']::INTEGER AS response_time_ms,\n        VALUE['snow.ai.observability.request_body']['messages'] AS messages_array,\n        CASE\n            WHEN VALUE['positive'] IS NOT NULL AND VALUE['positive'] = TRUE\n            THEN 'positive'\n            WHEN VALUE['positive'] IS NOT NULL AND VALUE['positive'] = FALSE\n            THEN 'negative'\n            ELSE 'no feedback'\n        END AS user_rating,\n        VALUE['feedback_message']::STRING AS user_feedback,\n        VALUE['dropdown_selection'] AS dropdown_selection,\n        VALUE\n    FROM ai_flat \n    WHERE event_name IN ('CORTEX_AGENT_REQUEST', 'CORTEX_AGENT_FEEDBACK')\n),\nrequest_data AS (\n    SELECT\n        b.*,\n        CASE \n            WHEN f.value['role']::STRING = 'user' \n            THEN f.value['content'][0]['text']::STRING \n        END AS user_prompt\n    FROM base_data b,\n    LATERAL FLATTEN(input => b.messages_array, outer => TRUE) f\n    WHERE b.event_name = 'CORTEX_AGENT_REQUEST'\n),\nfeedback_data AS (\n    SELECT\n        record_id,\n        user_rating,\n        user_feedback,\n        dropdown_selection,\n        timestamp AS feedback_timestamp\n    FROM base_data\n    WHERE event_name = 'CORTEX_AGENT_FEEDBACK'\n        AND user_rating != 'no feedback'\n),\nchat_level_data AS (\n    SELECT\n        r.conversation_id,\n        r.record_id,\n        r.agent_name,\n        r.username,\n        r.user_prompt,\n        r.ai_response,\n        r.timestamp AS chat_sent_time,\n        DATEADD('millisecond', r.response_time_ms, r.timestamp) AS ai_response_time,\n        COALESCE(f.user_rating, 'no feedback') AS user_rating,\n        f.user_feedback,\n        f.dropdown_selection,\n        r.response_time_ms,\n        r.observed_timestamp\n    FROM request_data r\n    LEFT JOIN feedback_data f \n        ON r.record_id = f.record_id\n    WHERE r.user_prompt IS NOT NULL \n        OR r.ai_response IS NOT NULL\n)\nSELECT\n    conversation_id,\n    record_id,\n    username,\n    agent_name,\n    chat_sent_time,\n    ai_response_time,\n    user_prompt,\n    ai_response,\n    user_rating,\n    user_feedback,\n    dropdown_selection,\n    response_time_ms,\n    DATE(chat_sent_time) AS chat_date,\n    HOUR(chat_sent_time) AS chat_hour,\n    DAYOFWEEK(chat_sent_time) AS day_of_week,\n    CASE \n        WHEN user_rating = 'positive' THEN 1 \n        WHEN user_rating = 'negative' THEN -1 \n        ELSE 0 \n    END AS rating_score,\n    ROW_NUMBER() OVER (PARTITION BY conversation_id ORDER BY chat_sent_time) AS message_sequence\nFROM chat_level_data\nORDER BY conversation_id, chat_sent_time DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00cd912a-3e5a-4b54-b46a-9db1a9d86b7a",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "SELECT * FROM AI_OBSERVABILITY_PROCESSED;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95b7759a-240c-4918-8e1b-66337eb0b955",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "PARSE_JSON(\n        '[\n    {\n      \"tool_name\": \"Sales_metrics_model\",\n      \"tool_sequence\": 1,\n      \"tool_input\": {\n        \"query\": \"How many deals did Sarah Johnson win compared to deals she lost\",\n        \"original_query\": \"How many deals did Sarah Johnson win compared to deals she lost?\",\n        \"previous_related_tool_result_id\": \"\",\n        \"check_metric_distribution\": \"include COUNT(*) as total_deals to verify we have complete data for Sarah Johnson\",\n        \"check_missing_data\": \"include MIN(CLOSE_DATE), MAX(CLOSE_DATE) to check the time range of Sarah Johnson''s deals\",\n        \"has_time_column\": true,\n        \"queried_time_period\": \"\"\n      },\n      \"tool_output\": {\n        \"analyst_latency_ms\": 2818,\n        \"analyst_orchestration_path\": \"regular_sqlgen\",\n        \"query_id\": \"01bfe03a-010b-963b-0000-8df925e1a17a\",\n        \"sql\": \"WITH __sales_metrics AS ( SELECT win_status, sales_rep, close_date FROM sales_intelligence.data.sales_metrics ) SELECT SUM(CASE WHEN win_status = TRUE THEN 1 ELSE 0 END) AS deals_won, SUM(CASE WHEN win_status = FALSE THEN 1 ELSE 0 END) AS deals_lost, COUNT(*) AS total_deals, MIN(close_date) AS min_close_date, MAX(close_date) AS max_close_date FROM __sales_metrics WHERE sales_rep = ''Sarah Johnson'' -- Generated by Cortex Analyst ;\",\n        \"sql_exec_result\": {\n          \"data\": [[\"2\",\"1\",\"3\",\"2024-Feb-10\",\"2024-Feb-15\"]],\n          \"resultSetMetaData\": {\n            \"format\": \"jsonv2\",\n            \"numRows\": 1,\n            \"partition\": 0,\n            \"rowType\": [\n              {\"name\":\"DEALS_WON\",\"type\":\"fixed\"},\n              {\"name\":\"DEALS_LOST\",\"type\":\"fixed\"},\n              {\"name\":\"TOTAL_DEALS\",\"type\":\"fixed\"},\n              {\"name\":\"MIN_CLOSE_DATE\",\"type\":\"date\"},\n              {\"name\":\"MAX_CLOSE_DATE\",\"type\":\"date\"}\n            ]\n          },\n          \"statementHandle\": \"01bfe03a-010b-963b-0000-8df925e1a17a\"\n        },\n        \"text\": \"The question is clear and I can answer it with the following SQL.\",\n        \"tool_result_id\": \"toolu_bdrk_01GATA1Fd86aMgQKCMeNZyKF\"\n      }\n    },\n    {\n      \"tool_name\": \"Sales_metrics_model\",\n      \"tool_sequence\": 2,\n      \"tool_input\": {\n        \"has_time_column\": false,\n        \"original_query\": \"How many deals did Sarah Johnson win compared to deals she lost?\",\n        \"previous_related_tool_result_id\": \"toolu_bdrk_01GATA1Fd86aMgQKCMeNZyKF\",\n        \"queried_time_period\": \"\",\n        \"query\": \"Show Sarah Johnson''s deals won vs deals lost in a format suitable for comparison chart with outcome type and count columns\",\n        \"check_metric_distribution\": \"\",\n        \"check_missing_data\": \"\"\n      },\n      \"tool_output\": {\n        \"analyst_latency_ms\": 2299,\n        \"analyst_orchestration_path\": \"regular_sqlgen\",\n        \"query_id\": \"01bfe03b-010b-9651-0000-8df925e1954a\",\n        \"sql\": \"WITH __sales_metrics AS ( SELECT win_status, sales_rep FROM sales_intelligence.data.sales_metrics ) SELECT CASE WHEN win_status = TRUE THEN ''Won'' ELSE ''Lost'' END AS outcome_type, COUNT(*) AS count FROM __sales_metrics WHERE sales_rep = ''Sarah Johnson'' GROUP BY win_status ORDER BY outcome_type -- Generated by Cortex Analyst ;\",\n        \"sql_exec_result\": {\n          \"data\": [[\"Lost\",\"1\"],[\"Won\",\"2\"]],\n          \"resultSetMetaData\": {\n            \"format\": \"jsonv2\",\n            \"numRows\": 2,\n            \"partition\": 0,\n            \"rowType\": [\n              {\"name\":\"OUTCOME_TYPE\",\"type\":\"text\"},\n              {\"name\":\"COUNT\",\"type\":\"fixed\"}\n            ]\n          },\n          \"statementHandle\": \"01bfe03b-010b-9651-0000-8df925e1954a\"\n        },\n        \"text\": \"The question is clear and I can answer it with the following SQL.\",\n        \"tool_result_id\": \"toolu_bdrk_01TvXETytnwx2hyB4Fbsiy4g\"\n      }\n    }\n  ]'\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d76baa9a-da98-4c55-af4c-350faa117121",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "#!/usr/bin/env python3\n\"\"\"\nFetch evaluation questions from AI Observability Event Table.\n\nThis script provides functions to query the event table and extract evaluation questions\nfrom the record_attributes JSON field. Questions can be deduplicated, filtered, and\noptionally include full trace data.\n\nCOMMAND-LINE USAGE:\n    python fetch_events_from_event_table.py <agent_name> <database> <schema> [OPTIONS]\n\nCOMMAND-LINE OPTIONS:\n    --connection CONNECTION     Snowflake connection name (default: snowhouse)\n    --output OUTPUT            Path to save questions as JSON (default: print to console)\n    --where WHERE_CLAUSE       Additional WHERE clause conditions\n    --limit N                  Maximum number of records to return\n    --unique                   Deduplicate questions by question text\n    --include-trace            Fetch complete trace for each record (slower)\n    --no-compress              Don't compress traces (only with --include-trace)\n\nCOMMAND-LINE EXAMPLES:\n    # Fetch all events for an agent:\n    python fetch_events_from_event_table.py RAVEN_SALES_ASSISTANT SNOWFLAKE_INTELLIGENCE AGENTS\n\n    # Fetch last 10 unique questions with compressed traces:\n    python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n        --where \"ORDER BY timestamp DESC\" --limit 10 --unique --include-trace\n\n    # Fetch traces uncompressed and save to file:\n    python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n        --include-trace --no-compress --output ./questions.json\n\n    # Get 20 questions from last 30 days:\n    python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n        --where \"timestamp > dateadd(day, -30, current_timestamp())\" --limit 20\n\nLIBRARY USAGE:\n    from fetch_events_from_event_table import fetch_events_from_event_table\n\n    # Get all questions:\n    questions = fetch_events_from_event_table('MY_AGENT', 'MY_DB', 'MY_SCHEMA')\n\n    # Get last 10 unique questions with compressed traces (default):\n    questions = fetch_events_from_event_table(\n        'MY_AGENT', 'MY_DB', 'MY_SCHEMA',\n        where_clause=\"ORDER BY timestamp DESC\",\n        limit=10,\n        unique=True,\n        include_trace=True\n    )\n\n    # Get traces uncompressed:\n    questions = fetch_events_from_event_table(\n        'MY_AGENT', 'MY_DB', 'MY_SCHEMA',\n        include_trace=True,\n        compress_trace=False\n    )\n\"\"\"\n\nimport json\nimport argparse\nimport snowflake.connector\nfrom typing import List, Dict, Any, Optional\n\nimport pandas as pd\n# from trulens.core.feedback.selector import Trace\n# from trulens.otel.semconv.trace import SpanAttributes\n\n\n# Event table constants\nEVENT_TABLE_DB = \"SNOWFLAKE\"\nEVENT_TABLE_SCHEMA = \"LOCAL\"\nEVENT_TABLE_FUNCTION = \"GET_AI_OBSERVABILITY_EVENTS\"\nEVENT_TABLE_NAME = \"AI_OBSERVABILITY_EVENTS\"\n\n\ndef escape_sql_string(value: str) -> str:\n    \"\"\"Escape single quotes in SQL string literals to prevent SQL injection.\"\"\"\n    return value.replace(\"'\", \"''\")\n\n\ndef fetch_trace_for_record(\n    cursor,\n    database: str,\n    schema: str,\n    agent_name: str,\n    record_id: str,\n    use_udtf: bool = True,\n) -> pd.DataFrame:\n    \"\"\"\n    Fetch all events (trace) for a given record_id as a DataFrame.\n\n    Args:\n        cursor: Snowflake cursor\n        database (str): Database where the agent is located\n        schema (str): Schema where the agent is located\n        agent_name (str): Name of the agent\n        record_id (str): The record ID to fetch trace for\n        use_udtf (bool): If True, use GET_AI_OBSERVABILITY_EVENTS UDTF; if False, use table directly\n\n    Returns:\n        pd.DataFrame: DataFrame containing all events in the trace, ordered by START_TIMESTAMP\n    \"\"\"\n    if use_udtf:\n        trace_sql = f\"\"\"\n        SELECT *\n        FROM TABLE({EVENT_TABLE_DB}.{EVENT_TABLE_SCHEMA}.{EVENT_TABLE_FUNCTION}(\n            '{escape_sql_string(database)}',\n            '{escape_sql_string(schema)}',\n            '{escape_sql_string(agent_name)}',\n            'CORTEX AGENT'\n        ))\n        WHERE RECORD_ATTRIBUTES:\"ai.observability.record_id\" = '{escape_sql_string(record_id)}'\n        ORDER BY START_TIMESTAMP ASC\n        \"\"\"\n    else:\n        # trace_sql = f\"\"\"\n        # SELECT *\n        # FROM {EVENT_TABLE_DB}.{EVENT_TABLE_SCHEMA}.{EVENT_TABLE_NAME}\n        # WHERE RECORD_ATTRIBUTES:\"snow.ai.observability.database.name\" = '{escape_sql_string(database)}'\n        # AND RECORD_ATTRIBUTES:\"snow.ai.observability.schema.name\" = '{escape_sql_string(schema)}'\n        # AND RECORD_ATTRIBUTES:\"snow.ai.observability.object.name\" = '{escape_sql_string(agent_name)}'\n        # AND RECORD_ATTRIBUTES:\"ai.observability.record_id\" = '{escape_sql_string(record_id)}'\n        # ORDER BY START_TIMESTAMP ASC\n        # \"\"\"\n        raise ValueError(\n            \"UDTF must be True, direct table query is experimental and not recommended\"\n        )\n\n    cursor.execute(trace_sql)\n\n    # Get column names\n    column_names = [desc[0].lower() for desc in cursor.description]\n\n    # Fetch all trace spans\n    trace_rows = cursor.fetchall()\n\n    # Convert to list of dicts\n    trace_spans = []\n    for row in trace_rows:\n        row_dict = dict(zip(column_names, row))\n\n        # Parse JSON fields\n        for json_field in [\n            \"record\",\n            \"record_attributes\",\n            \"trace\",\n            \"resource_attributes\",\n        ]:\n            if json_field in row_dict and row_dict[json_field]:\n                try:\n                    if isinstance(row_dict[json_field], str):\n                        row_dict[json_field] = json.loads(row_dict[json_field])\n                except json.JSONDecodeError:\n                    pass  # Keep as string if not valid JSON\n\n        # Convert timestamps to ISO format strings\n        for ts_field in [\"start_timestamp\", \"timestamp\"]:\n            if ts_field in row_dict and row_dict[ts_field]:\n                row_dict[ts_field] = str(row_dict[ts_field])\n\n        trace_spans.append(row_dict)\n\n    # Return as DataFrame\n    return pd.DataFrame(trace_spans)\n\n\ndef fetch_events_from_event_table(\n    agent_name: str,\n    database: str,\n    schema: str,\n    connection_name: str = \"snowhouse\",\n    where_clause: Optional[str] = None,\n    limit: Optional[int] = None,\n    unique: bool = False,\n    include_trace: bool = False,\n    compress_trace: bool = True,\n    use_udtf: bool = True,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Fetch evaluation questions from the AI Observability Event Table.\n\n    This function queries the event table using GET_AI_OBSERVABILITY_EVENTS and extracts\n    questions and ground truth answers from the record_attributes JSON field using\n    TruLens SpanAttributes constants (RECORD_ROOT.INPUT and RECORD_ROOT.GROUND_TRUTH_OUTPUT).\n\n    By default, returns all events without deduplication. Set unique=True to deduplicate by question text.\n\n    Args:\n        agent_name (str): Name of the agent to fetch events for\n        database (str): Database where the agent is located\n        schema (str): Schema where the agent is located\n        connection_name (str): Snowflake connection name (default: \"snowhouse\")\n        where_clause (Optional[str]): Additional conditions to filter results (joined with AND).\n                                      Can include ORDER BY clauses.\n                                      Examples:\n                                        - \"timestamp > '2025-01-01'\"\n                                        - \"ORDER BY timestamp DESC\"\n                                      Note: Don't include \"WHERE\" or \"AND\" at the beginning of the string - they're added automatically\n        limit (Optional[int]): Maximum number of records to return.\n                              When unique=False, limits SQL results.\n                              When unique=True, limits after deduplication to ensure exact count.\n                              Example: limit=10 with unique=True returns exactly 10 unique questions\n        unique (bool): If True, deduplicate questions by question text (default: False).\n                      When True, keeps first occurrence of each unique question.\n        include_trace (bool): If True, fetch complete trace (all spans) for each record and include\n                             as JSON string in 'trace' field (default: False).\n                             Warning: This makes N+1 queries and can be slow for many records.\n        compress_trace (bool): If True (default), compress trace JSON using TruLens compression.\n                              If False, return full uncompressed trace JSON.\n                              Only used when include_trace=True.\n        use_udtf (bool): If True (default), use GET_AI_OBSERVABILITY_EVENTS UDTF; if False, query table directly.\n                        NOTE: UDTF requires proper authorization (USAGE privilege on the agent).\n                        Use --no-udtf flag if you get \"does not exist or not authorized\" errors.\n                        Direct table approach is more permissive and works for archived/deleted agents.\n\n    Returns:\n        list[dict]: List of question dictionaries, each containing:\n                   - question (str): The question text\n                   - answer (str): The actual answer from the agent\n                   - ground_truth (str): The expected answer (ground truth)\n                   - record_id (str): The record ID for this question/answer\n                   - trace (str): JSON string of trace events (only if include_trace=True).\n                                 Compressed using TruLens if compress_trace=True (default),\n                                 or uncompressed if compress_trace=False\n\n    Example:\n        # Basic usage:\n        questions = fetch_events_from_event_table(\n            agent_name=\"RAVEN_SALES_ASSISTANT\",\n            database=\"SNOWFLAKE_INTELLIGENCE\",\n            schema=\"AGENTS\"\n        )\n\n        # Get last 10 unique questions with compressed traces (default):\n        questions = fetch_events_from_event_table(\n            agent_name=\"MY_AGENT\",\n            database=\"MY_DB\",\n            schema=\"MY_SCHEMA\",\n            where_clause=\"ORDER BY timestamp DESC\",\n            limit=10,\n            unique=True,\n            include_trace=True\n        )\n\n        # Get traces uncompressed:\n        questions = fetch_events_from_event_table(\n            agent_name=\"MY_AGENT\",\n            database=\"MY_DB\",\n            schema=\"MY_SCHEMA\",\n            include_trace=True,\n            compress_trace=False\n        )\n\n        # Get 20 questions from last 30 days (may include duplicates):\n        questions = fetch_events_from_event_table(\n            agent_name=\"MY_AGENT\",\n            database=\"MY_DB\",\n            schema=\"MY_SCHEMA\",\n            where_clause=\"timestamp > dateadd(day, -30, current_timestamp())\",\n            limit=20\n        )\n    \"\"\"\n    conn = snowflake.connector.connect(connection_name=connection_name)\n    try:\n        cursor = conn.cursor()\n\n        # Build SQL query\n        # NOTE: UDTF validates agent existence and authorization at query time.\n        # If agent doesn't exist or user lacks permissions, query will fail.\n        # Table approach (default) bypasses these checks and queries historical data directly.\n        if use_udtf:\n            base_sql = f\"\"\"\n            SELECT *\n            FROM TABLE({EVENT_TABLE_DB}.{EVENT_TABLE_SCHEMA}.{EVENT_TABLE_FUNCTION}(\n                '{escape_sql_string(database)}',\n                '{escape_sql_string(schema)}',\n                '{escape_sql_string(agent_name)}',\n                'CORTEX AGENT'\n            ))\n            WHERE RECORD_ATTRIBUTES:\"ai.observability.span_type\" = 'record_root'\n            \"\"\"\n        else:\n            # base_sql = f\"\"\"\n            # SELECT *\n            # FROM {EVENT_TABLE_DB}.{EVENT_TABLE_SCHEMA}.{EVENT_TABLE_NAME}\n            # WHERE RECORD_ATTRIBUTES:\"snow.ai.observability.database.name\" = '{escape_sql_string(database)}'\n            # AND RECORD_ATTRIBUTES:\"snow.ai.observability.schema.name\" = '{escape_sql_string(schema)}'\n            # AND RECORD_ATTRIBUTES:\"snow.ai.observability.object.name\" = '{escape_sql_string(agent_name)}'\n            # AND RECORD_ATTRIBUTES:\"ai.observability.span_type\" = 'record_root'\n            # \"\"\"\n            raise ValueError(\n                \"UDTF must be True, direct table query is experimental and not recommended\"\n            )\n\n        # Add additional conditions if provided\n        if where_clause:\n            # Check if where_clause starts with ORDER BY (no AND needed)\n            if where_clause.strip().upper().startswith(\"ORDER BY\"):\n                sql = f\"{base_sql} {where_clause}\"\n            else:\n                sql = f\"{base_sql} AND {where_clause}\"\n        else:\n            sql = base_sql\n\n        # Add LIMIT if specified (only when not deduplicating, otherwise apply after dedup)\n        if limit is not None and limit > 0 and not unique:\n            sql = f\"{sql} LIMIT {limit}\"\n\n        print(f\"Querying event table for agent: {database}.{schema}.{agent_name}\")\n        cursor.execute(sql)\n\n        # Get column names from cursor description\n        column_names = [desc[0].lower() for desc in cursor.description]\n\n        # Process rows and extract questions\n        rows = cursor.fetchall()\n\n        print(f\"Retrieved {len(rows)} event records\")\n        print(f\"Columns: {', '.join(column_names)}\")\n\n        if unique:\n            # Deduplicate by question text\n            questions_dict = {}\n            for row in rows:\n                try:\n                    # Convert row to dict using column names\n                    row_dict = dict(zip(column_names, row))\n\n                    # Parse record_attributes JSON\n                    record_attributes_str = row_dict.get(\"record_attributes\")\n\n                    # Handle both string and already-parsed JSON\n                    if isinstance(record_attributes_str, str):\n                        record_attributes = json.loads(record_attributes_str)\n                    else:\n                        record_attributes = record_attributes_str\n\n                    # Extract fields from record_attributes\n                    question = record_attributes.get(SpanAttributes.RECORD_ROOT.INPUT)\n                    answer = record_attributes.get(SpanAttributes.RECORD_ROOT.OUTPUT)\n                    ground_truth = record_attributes.get(\n                        SpanAttributes.RECORD_ROOT.GROUND_TRUTH_OUTPUT\n                    )\n                    record_id = record_attributes.get(SpanAttributes.RECORD_ID)\n\n                    # Skip if no question\n                    if not question:\n                        continue\n\n                    # Skip duplicates (keep first occurrence)\n                    if question in questions_dict:\n                        continue\n\n                    # Build question data\n                    question_data = {\n                        \"question\": question,\n                        \"answer\": answer,\n                        \"ground_truth\": ground_truth,\n                        \"record_id\": record_id,\n                    }\n\n                    questions_dict[question] = question_data\n\n                except (json.JSONDecodeError, KeyError) as e:\n                    print(f\"Warning: Failed to parse event record: {e}\")\n                    continue\n\n            # Convert dict to list\n            questions = list(questions_dict.values())\n\n            # Apply limit after deduplication\n            if limit is not None and limit > 0:\n                questions = questions[:limit]\n\n            print(f\"Extracted {len(questions)} unique questions from event table\")\n        else:\n            # No deduplication\n            questions = []\n            for row in rows:\n                try:\n                    # Convert row to dict using column names\n                    row_dict = dict(zip(column_names, row))\n\n                    # Parse record_attributes JSON\n                    record_attributes_str = row_dict.get(\"record_attributes\")\n\n                    # Handle both string and already-parsed JSON\n                    if isinstance(record_attributes_str, str):\n                        record_attributes = json.loads(record_attributes_str)\n                    else:\n                        record_attributes = record_attributes_str\n\n                    # Extract fields from record_attributes\n                    question = record_attributes.get(SpanAttributes.RECORD_ROOT.INPUT)\n                    answer = record_attributes.get(SpanAttributes.RECORD_ROOT.OUTPUT)\n                    ground_truth = record_attributes.get(\n                        SpanAttributes.RECORD_ROOT.GROUND_TRUTH_OUTPUT\n                    )\n                    record_id = record_attributes.get(SpanAttributes.RECORD_ID)\n\n                    # Skip if no question\n                    if not question:\n                        continue\n\n                    # Build question data\n                    question_data = {\n                        \"question\": question,\n                        \"answer\": answer,\n                        \"ground_truth\": ground_truth,\n                        \"record_id\": record_id,\n                    }\n\n                    questions.append(question_data)\n\n                except (json.JSONDecodeError, KeyError) as e:\n                    print(f\"Warning: Failed to parse event record: {e}\")\n                    continue\n\n            print(f\"Extracted {len(questions)} questions from event table\")\n\n        # Fetch traces if requested\n        if include_trace:\n            print(f\"\\nFetching traces for {len(questions)} records...\")\n            for i, question in enumerate(questions):\n                record_id = question.get(\"record_id\")\n                if record_id:\n                    try:\n                        events_df = fetch_trace_for_record(\n                            cursor=cursor,\n                            database=database,\n                            schema=schema,\n                            agent_name=agent_name,\n                            record_id=record_id,\n                            use_udtf=use_udtf,\n                        )\n\n                        if not events_df.empty:\n                            trace = Trace()\n                            trace.events = events_df\n\n                            if compress_trace:\n                                trace_json = trace.to_compressed_json(\n                                    default_handler=str\n                                )\n                            else:\n                                trace_json = (\n                                    trace.events.to_json(default_handler=str)\n                                    if trace.events is not None\n                                    else \"{}\"\n                                )\n                            question[\"trace\"] = trace_json\n                        else:\n                            question[\"trace\"] = \"{}\"\n\n                        if (i + 1) % 10 == 0:\n                            print(f\"  Fetched {i + 1}/{len(questions)} traces...\")\n                    except Exception as e:\n                        print(\n                            f\"Warning: Failed to fetch trace for record_id {record_id}: {e}\"\n                        )\n                        question[\"trace\"] = None\n                else:\n                    question[\"trace\"] = None\n            print(\"✓ Completed fetching traces\")\n\n        return questions\n\n    finally:\n        cursor.close()\n        conn.close()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1f28630-411c-438f-b806-84a9ed34942d",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Fetch evaluation questions from AI Observability Event Table\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Fetch all events:\n  python fetch_events_from_event_table.py RAVEN_SALES_ASSISTANT SNOWFLAKE_INTELLIGENCE AGENTS\n\n  # Fetch last 10 unique questions with traces (compressed by default):\n  python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n      --where \"ORDER BY timestamp DESC\" --limit 10 --unique --include-trace\n\n  # Fetch traces uncompressed:\n  python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n      --include-trace --no-compress\n\n  # Save to file:\n  python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n      --connection snowhouse --output ./questions.json\n\n  # Get 20 questions from last 30 days:\n  python fetch_events_from_event_table.py MY_AGENT MY_DB MY_SCHEMA \\\\\n      --where \"timestamp > dateadd(day, -30, current_timestamp())\" --limit 20\n        \"\"\",\n    )\n\n    # Required arguments\n    parser.add_argument(\"agent_name\", help=\"Name of the agent to fetch events for\")\n    parser.add_argument(\"database\", help=\"Database where agent is located\")\n    parser.add_argument(\"schema\", help=\"Schema where agent is located\")\n\n    # Optional arguments\n    parser.add_argument(\n        \"--connection\",\n        default=\"snowhouse\",\n        help=\"Snowflake connection name (default: snowhouse)\",\n    )\n    parser.add_argument(\n        \"--output\", help=\"Path to save questions as JSON (default: print to console)\"\n    )\n    parser.add_argument(\n        \"--where\",\n        help=\"Additional WHERE clause conditions (don't include 'WHERE' or 'AND')\",\n    )\n    parser.add_argument(\"--limit\", type=int, help=\"Maximum number of records to return\")\n    parser.add_argument(\n        \"--unique\", action=\"store_true\", help=\"Deduplicate questions by question text\"\n    )\n    parser.add_argument(\n        \"--include-trace\",\n        action=\"store_true\",\n        help=\"Fetch complete trace for each record (slower)\",\n    )\n    parser.add_argument(\n        \"--no-compress\",\n        action=\"store_true\",\n        help=\"Don't compress traces (only with --include-trace)\",\n    )\n    parser.add_argument(\n        \"--no-udtf\",\n        action=\"store_false\",\n        dest=\"use_udtf\",\n        help=\"Query event table directly instead of using GET_AI_OBSERVABILITY_EVENTS UDTF (use if you get authorization errors)\",\n    )\n\n    args = parser.parse_args()\n\n    print(\"\\nFetching Events from AI Observability Event Table\")\n    print(\"=\" * 80)\n    print(f\"Agent: {args.database}.{args.schema}.{args.agent_name}\")\n    print(f\"Connection: {args.connection}\")\n    if args.where:\n        print(f\"Filter: {args.where}\")\n    if args.limit:\n        print(f\"Limit: {args.limit}\")\n    if args.unique:\n        print(\"Mode: Unique questions only\")\n    if args.include_trace:\n        compress_mode = \"uncompressed\" if args.no_compress else \"compressed\"\n        print(f\"Traces: Enabled ({compress_mode})\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # Fetch questions\n    questions = fetch_events_from_event_table(\n        agent_name=args.agent_name,\n        database=args.database,\n        schema=args.schema,\n        connection_name=args.connection,\n        where_clause=args.where,\n        limit=args.limit,\n        unique=args.unique,\n        include_trace=args.include_trace,\n        compress_trace=not args.no_compress,\n        use_udtf=args.use_udtf,\n    )\n\n    # Output results\n    if args.output:\n        with open(args.output, \"w\") as f:\n            json.dump(questions, f, indent=2)\n        print(f\"\\n✓ Saved {len(questions)} questions to: {args.output}\")\n    else:\n        print(f\"\\n{'='*80}\")\n        print(\"QUESTIONS\")\n        print(\"=\" * 80 + \"\\n\")\n        print(json.dumps(questions, indent=2))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad0d40c5-249b-4989-8cc3-9ec329c9c90d",
   "metadata": {
    "name": "ARCHIVE_BELOW",
    "collapsed": false
   },
   "source": "# ARCHIVE BELOW"
  },
  {
   "cell_type": "code",
   "id": "4a0e5ee9-1c45-434a-aac4-ff9e6b4d2456",
   "metadata": {
    "language": "sql",
    "name": "see_scopes"
   },
   "outputs": [],
   "source": "SELECT SCOPE:name, COUNT(*) FROM SNOWFLAKE.LOCAL.AI_OBSERVABILITY_EVENTS \n-- WHERE\n-- ORDER BY TIMESTAMP DESC\nGROUP BY SCOPE\n-- limit 10;",
   "execution_count": null
  }
 ]
}